

<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ExpressivityArena: Can LLMs Express Information Implicitly?</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Roboto', sans-serif;
            margin: 0;
            padding: 0;
            color: #333;
        }
        .header {
            background-color: #4a90e2;
            color: #fff;
            padding: 20px;
            text-align: center;
        }
        .content {
            max-width: 800px;
            margin: auto;
            padding: 20px;
        }
        h1 {
            color: #ffffff;
        }
        h2 {
            color: #4a90e2;
        }
        .key-features, .results {
            margin-bottom: 20px;
        }
        .feature-item, .result-item {
            background: #f9f9f9;
            padding: 10px;
            border: 1px solid #ddd;
            margin-top: 10px;
            border-radius: 5px;
        }
        .citation {
            background: #e8f4fc;
            padding: 15px;
            margin-top: 20px;
            border-left: 4px solid #4a90e2;
        }
        footer {
            text-align: center;
            padding: 20px;
            font-size: 0.9em;
            color: #555;
            background: #f2f2f2;
        }
        pre {
            background: #f9f9f9;
            padding: 10px;
            border-radius: 5px;
            border: 1px solid #ddd;
            overflow-x: auto;
        }
    </style>
</head>
<body>

<div class="header">
    <h1>ExpressivityArena: Can LLMs Express Information Implicitly?</h1>
    <p>Joshua Tint, Som Sagar, Aditya Taparia, Kelly Raines, Bimsara Pathiraja, Caleb Liu, Ransalu Senanayake</p>
    <a href="/assets/expressivityarena.pdf" download="ExpressivityArena_Paper.pdf">
        <button style="background-color:#4a90e2; color:white; padding:10px 20px; border:none; border-radius:5px; cursor:pointer;">
            Download Paper
        </button>
    </a>

    <a href="/assets/ExpressivityArena.zip" download="ExpressivityArena.zip">
        <button style="background-color:#4a90e2; color:white; padding:10px 20px; border:none; border-radius:5px; cursor:pointer;">
            Download Code
        </button>
    </a>
</div>

<div class="content">
    <img src="/assets/fig1.png" alt="Figure 1" style="width: 100%; border-radius: 5px; margin-bottom: 20px;">

    
    <section class="introduction">
        <h2>Abstract</h2>
        <p>While Large Language Models (LLMs) have demonstrated remarkable performance in certain dimensions, their ability to express implicit language cues that human use for effective communication remains unclear. This paper presents ExpressivityArena, a Python library for measuring the implicit communication abilities of LLMs. We provide a comprehensive framework to evaluate expressivity of arbitrary LLMs and explore its practical implications. To this end, we refine the definition and measurements of “expressivity,” and use our framework in a set of small experiments. These experiments test LLMs in creative and logical tasks such as poetry, coding, and emotion-based responses. They are then evaluated by an automated grader, through ExpressivityArena, which we verify to be the most pragmatic for testing expressivity. Building on these experiments, we deepen our understanding of the expressivity of LLMs by assessing their ability to remain expressive in conversations. Our findings indicate that LLMs are capable of generating and understanding expressive content, however, with some limitations. These insights will inform the future development and deployment of expressive LLMs. We provide the code for ExpressivityArena alongside our paper.</p>
    </section>


    <div class="content">
        <section class="key-features">
            <h2>Key Features</h2>
            <div class="feature-item">
                <strong>Expressivity Evaluation Framework:</strong> A Python library developed to test Large Language Models' (LLMs) ability to convey implicit information in text.
            </div>
            <div class="feature-item">
                <strong>Automated Grader:</strong> Utilizes an automated grading system for efficiently testing LLM responses across various tasks like poetry and code generation.
            </div>
            <div class="feature-item">
                <strong>Diverse Experiment Domains:</strong> Evaluates LLMs on both high-expressivity tasks (e.g., poetry) and low-expressivity tasks (e.g., code generation).
            </div>
        </section>

        <section class="results">
            <h2>Key Results</h2>
            <div class="result-item">
                <strong>LLM Performance in Poetry vs. Code:</strong> LLMs performed significantly better in expressive tasks like poetry than in more functional tasks like code generation, suggesting limitations in conveying expressive intent in programming.
            </div>
            <div class="result-item">
                <strong>Conversational Expressivity:</strong> LLMs could maintain expressive cues over the course of simulated conversations, with a consistent expressivity increase for profession-based signals but a decrease over time for emotion-based signals.
            </div>
            <div class="result-item">
                <strong>Automated Grader Accuracy:</strong> The automated grader performed comparably to human evaluators, validating its use in assessing LLM-generated text expressivity.
            </div>
        </section>

    <section class="components">
        <h2>Components</h2>
        <div class="feature-item">
            <strong>LLM (Language Model):</strong>
            <p>The <code>LLM</code> class represents the functionality of a Language Model.</p>
            <ul>
                <li><code>name</code>: The name of the LLM for identification purposes.</li>
                <li><code>get_response</code>: A function representing the prompt -> response functionality of the LLM.</li>
            </ul>
        </div>
        <div class="feature-item">
            <strong>ExpressivityPrompt:</strong>
            <p>The <code>ExpressivityPrompt</code> class encapsulates a prompt to produce a response conveying a particular signal in a specific context.</p>
            <ul>
                <li><code>instruction</code>: The type of response to generate, such as "poem," "speech," or "Python program."</li>
                <li><code>expressivity_signal</code>: The signal to encode into the response, such as "sad," "secretive," or "well-educated."</li>
            </ul>
        </div>
        <div class="feature-item">
            <strong>ExpressivityResult:</strong>
            <p>The <code>ExpressivityResult</code> class represents the result of an ExpressivityArena experiment.</p>
            <ul>
                <li><code>LLM</code>: The LLM used for the experiment.</li>
                <li><code>prompt</code>: The prompt used for the experiment.</li>
                <li><code>result</code>: The response the LLM gave for the prompt.</li>
                <li><code>grade</code>: A boolean indicating whether the response effectively expressed the signal.</li>
            </ul>
        </div>
        <div class="feature-item">
            <strong>Grader:</strong>
            <p>The <code>Grader</code> class is a base class for grading whether responses express a given signal or not. Various grader schemas are offered.</p>
        </div>
    </section>

    <section class="usage">
        <h2>Usage</h2>
        <h3>Evaluation</h3>
        <p>The <code>evaluate</code> function evaluates an ExpressivityArena prompt to produce an expressive result and then grades the expressivity of that response.</p>
        <pre>
evaluate(llm: LLM, grader: Grader, prompt: ExpressivityPrompt) -> ExpressivityResult
            </pre>

        <h3>Batch Evaluation</h3>
        <p>The <code>batch_evaluate</code> function returns a list of ExpressivityArena experiment results for a list of prompts.</p>
        <pre>
evaluate(llm: LLM, grader: Grader, prompts: List[ExpressivityPrompt]) -> List[ExpressivityResult]
            </pre>
    </section>

    <section class="example">
        <h2>Example</h2>
        <pre>
from arena import evaluate_category
from context import SignalCategory
from grader import MultipleChoiceGrader
from llm import LLM

# Initialize LLM and Grader
def gpt_response_function(prompt: str):
    # Fetch a response for the given prompt from the GPT API...
    return response

llm = LLM(name="GPT", get_response=gpt_response_function)
grader = MultipleChoiceGrader(llm)

# Define signals
genres_category = SignalCategory("genres", [
        "horror", "romance", "thriller", "comedy", "drama"
])

# Evaluate prompts
results = evaluate_category(llm, grader, "short story", genres_category)

# Process results
for result in results:
    print(f"Prompt: {result.prompt}")
    print(f"Result: {result.result}")
    print(f"Grade: {'Expressive' if result.grade else 'Not Expressive'}")
    print()
            </pre>
    </section>

    <section class="citation">
        <h2>Reference</h2>
        <p>Joshua Tint, Som Sagar, Aditya Taparia, Kelly Raines, Bimsara Pathiraja, Caleb Liu, Ransalu Senanayake. "<em>ExpressivityArena: Can LLMs Express Information Implicitly?</em>" In Proceedings of NAACL 2024. <br><br>
            BibTeX Citation:
        <pre>
@misc{Tint2024ExpressivityArena,
    author = {Joshua Tint and Som Sagar and Aditya Taparia and Kelly Raines and Bimsara Pathiraja and Caleb Liu and Ransalu Senanayake},
    title = {ExpressivityArena: Can LLMs Express Information Implicitly?},
    year = {2024}
}
            </pre>
        </p>
    </section>
</div>

<footer>
    &copy; 2024 ExpressivityArena. All rights reserved.
</footer>

</body>
</html>
